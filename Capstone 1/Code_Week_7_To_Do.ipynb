{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# CLassification Problem Matrics - \n",
    "\n",
    "Write down the thought process for each metric and how you are thinking about the metric and the business problem.\n",
    "# Confusion Matrix  - Y\n",
    "# Accuracy - Y (Accuracy can be used when two classes are almost close 60-40%. But in our case it is 90-10%) - \n",
    "# Upsample the under representative class. (YES)\n",
    "\n",
    "# Precision - Y\n",
    "# Sensitivity or Recall  - Y\n",
    "# Specificity - Y\n",
    "# F1 Score\n",
    "# CART Algorithm\n",
    "# Gain and Lift Chart\n",
    "# Kolmogorov Smirnov Chart\n",
    "# AUC – ROC - Y\n",
    "# Gini Coefficient\n",
    "# Concordant – Discordant Ratio\n",
    "# Root Mean Squared Error (NOT FOR CLASSIFICATION BUT FOR PREDICTION)\n",
    "# Cross Validation (Not a metric though!)\n",
    "\n",
    "============================================================================================================================\n",
    "# I still have to get used to the terminology (TP, TN, FP, FN)\n",
    "# Also, I struggle to differentiate the different metrics. \n",
    "# (precision - classifiers performace w.r.to false positives\n",
    "\n",
    "Business objective is important. We decide on our metrics based on the business problem in hand.\n",
    "No matter what metric we choose, we primarily focus on minimizing Type I and Type II error (FF or FN)\n",
    "But how do we do it? To understand this, you need to be familiar with how each metric works and how each differ from one another.\n",
    "\n",
    "We cannot always acheive best of all metrics. There is always the tradeoffs. We have to choose our tradeoffs based on what\n",
    "we are trying to minimize.\n",
    "Type I error and Type II. \n",
    "\n",
    "# When to use what metrics? No matter which blog I read, they keep saying the respective 'metrics' which they wrote the the \n",
    "# \"BEST 5\", \"IMPORTANT 5\" and so no.... How do I know which metric to use when.\n",
    "# AS you keep working on various business problems, you get to gain a sense of intuition as to which of these tradeoffs\n",
    "# actually matter.\n",
    "\n",
    "Next Week\n",
    "\n",
    "- Understand each variable and how the affect the business and if they are important (If imp, why and how) \n",
    "    [Make a note of all this] - [DONE]\n",
    "- Machine Learning (Classification, KNN, Random Forests, SVM)  - [HALFWAY]\n",
    "- Get familiar with the terminology - [DONE]\n",
    "- Get familiar with the metrics taht we are using for this business problem - [DONE]\n",
    "- Get familiar with the other metrics. - [DONE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We have confusion matrix for a binary classification problem. But what if we had a multi-category classification problem?\n",
    "\n",
    "# TP - Model predicted yes, when customer said yes\n",
    "# TN - Model predicted no, when customer said no\n",
    "# FP - Model predicted yes, when customer said no [Reject NH when True - FP]\n",
    "# FN - Model predicted no, when customer said yes [Accept NH when False - FN]\n",
    "\n",
    "# We focus on minimizing FN for our problem. Since our goal is to minimize FN (how many customers did we miss), we need to \n",
    "# get our 'Recall' to be as close to 100% as possible without precision being too bad.\n",
    "\n",
    "AUC/ROC rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In statistics there are type I errors and type II errors. Relative to true positive and false positive terminology, \n",
    "# a type I error occurs when you reject the null hypothesis (as false) when it is actually true, which by convention \n",
    "# corresponds to a false positive. A type II error occurs when you accept the null hypothesis (as true) when it is actually \n",
    "# false, which by convention corresponds to a false negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "5/23/2018\n",
    "\n",
    "(a) Practise confusion matrix and classification metrics for various classification problem\n",
    "(b) ML - Classification, Random Forest, SVM\n",
    "(c) Classification Metrics Notes\n",
    "(d) Feature Engineering - Feature Selection NOtes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
